# The Outcome-Based Agile Framework

In a world where agile has often been diluted into process rituals
without purpose, the *Outcome-Based Agile Framework* reclaims its core
intent: to create meaningful change through adaptive learning and
value delivery. Inspired by the original
[Agile Manifesto](https://agilemanifesto.org/), this framework defines
a model where teams are driven by **outcomes**, **not requirements**,
and guided by evidence, not assumption.

> **Common Misreadings**
>
> * **No Upfront Requirements** does **not** mean chaos. It means starting
>   with **intent**, not **specification**.
> * **Governance exists to support autonomy** does **not** imply lack
>   of structure. It means governance should **enable**, not
>   **control**.

## Manifesto

We are uncovering better ways of creating value by focusing on
outcomes rather than outputs. Through this work, we have come to
value:

- **Outcomes over requirements**  
Because solving problems matters more than delivering predetermined
solutions.
- **Constraints over scope**  
Because framing the problem is more productive than prescribing its
answer.
- **Discovery over certainty**  
Because we can’t predict value—we must find it through learning.
- **Accountability to results over compliance to plans**  
Because teams should be responsible for making change, not just
delivering work.
- **Alignment on intent over agreement on features**  
Because shared goals outlast specific ideas.

That is, while there is value in outputs, defined scopes, and planned
features,  
**we value the ability to learn and adapt toward real outcomes far
more.**

## Framework Tenets

### 1. No Upfront Requirements

Work does not begin with a list of features, specifications, or
outputs. Teams begin with problems to solve and outcomes to
achieve. The only fixed inputs are constraints.

### 2. Clarify the Purpose of the Outcome

Ensure the team understands why the outcome matters—its strategic,
operational, or user-centered importance—before starting the
work. Quality attributes often define what "good" looks like for a
given outcome. Teams should clarify which attributes (e.g., usability,
reliability, performance, security) are essential for success, and
ensure those are explicitly part of the intended impact.

### 3. Problem Ownership Over Task Execution

Teams are given ownership of a problem and trusted to determine how
best to solve it within the defined boundaries.

### 4. Outcomes Are Observable Changes

An outcome is a measurable shift in user behavior, business value, or
system performance. Delivery is only valuable if it contributes to
these shifts. This includes improvements in non-functional areas like
usability, system reliability, deployment efficiency, and
security. Quality attributes must be observable and verifiable as part
of outcome validation.

### 5. Constraints Define Limits, Not Solutions

Constraints—technical, legal, ethical, or otherwise—are real and
respected. They inform exploration but must not dictate the final form
of a solution.

### 6. Continuous Discovery Is Mandatory

Discovery and delivery happen together. Teams explore problems, test
assumptions (solution hypotheses), and validate ideas in real
time. This includes discovering which quality attributes matter most
in the context and validating them with users or stakeholders
continuously.

### 7. Evidence Is the Arbiter

All decisions are made based on learning from real users and
data. **Plans are hypotheses**, not contracts.

### 8. Strategy Is Intent, Not Instruction

Leaders set direction through vision and desired outcomes, not through
detailed feature roadmaps.

### 9. Simplicity of Governance

Governance exists to support autonomy and learning. It must be light,
minimal, and enabling—never controlling.

### 10. Regular Blameless Retrospectives Through After Action Reviews (AAR)

Teams must conduct regular retrospectives at intervals appropriate to
their context (after iterations, deliveries, daily, weekly, or
combinations). The recommended format is the *After Action Review*
(AAR), which uses four blameless and structured questions:

- **What was supposed to happen?** (What was planned?)
- **What happened?** (What actually occurred?)
- **Why did it happen?** (Root causes and contributing factors)
- **What did we learn?** (Key insights and adaptations)

Each question is discussed separately and sequentially by the entire
team. Conversations about "what was planned," "what happened," and
"why" must be isolated from each other to avoid conflating facts and
analysis. Learning points can be documented at any time in the "what
did we learn" section.

Differences in team members' understanding are celebrated as learning
opportunities rather than treated as errors. The goal is not for all
team members to have identical views but to explore the richness of
different perspectives.

It is considered rude to interrupt team members while they are
speaking. The facilitator should ensure that each team member has the
opportunity to speak without interruption and can finish sharing their
thoughts before others respond.

Importantly, the "what did we learn" section, including any optional
recording in a knowledge base, must never include the names of
individual team members. Only lessons learned, without attribution to
specific people, are captured to ensure a fully blameless environment
that fosters openness and continuous improvement.

## How to Apply This Framework

> **Start small:** Before applying the framework to any
> initiative—whether one, many, or all—begin with the Readiness
> Assessment in the next section. It will help you evaluate whether
> your teams, culture, and leadership are prepared for outcome-driven
> ways of working. The results will highlight strengths to build on,
> surface gaps to address, and guide a pace of adoption that fits your
> context—ensuring the framework supports rather than overwhelms.

Organizations adopting Outcome-Based Agile should begin by clearly
defining the outcomes they seek. These outcomes should not only be
real, measurable changes in user or business behavior but also have a
clearly articulated purpose. Clarifying *why* each outcome matters
helps ensure alignment across teams, guides prioritization, and
connects day-to-day work to broader organizational goals.

In defining and pursuing outcomes, teams should explicitly consider
which quality attributes are critical to achieving the desired
change. These attributes—such as security, suitability, usability,
reliability, maintainability, compatibility, or deployment
efficiency—must be treated as part of the outcome, not as secondary
requirements. Quality attributes should be framed as hypotheses to
test, measured as part of delivery, and evaluated through direct
feedback and evidence, not assumed as implicit side effects of feature
development.

Teams should be cross-functional, capable of discovery and delivery,
and given end-to-end responsibility for their outcomes.

**Roadmaps become hypothesis backlogs**. Requirements become
constraints. Progress is measured not by story points, but by progress
toward the desired change.

Leadership must shift from steering work to enabling learning. Teams
must shift from implementing solutions to exploring possibilities. And
the organization must create space for evidence to matter more than
certainty.

### Outcome Cards

To make outcomes actionable, teams and leaders should express each one
as an **Outcome Card**—a structured, lightweight artifact that
captures intent, ownership, signals, and constraints in a single view.

#### What Is an Outcome Card?

An **Outcome Card** defines a desired change, not a task. It helps
teams understand:

* What matters
* Why it matters
* What boundaries to respect
* How success might be observed

Outcome Cards guide discovery, align stakeholders, and ensure clarity
without prescribing solutions.

#### Anatomy of an Outcome Card

Each card should include the following elements:

| Field                  | Description                                                                                                     |
| ---------------------- | --------------------------------------------------------------------------------------------------------------- |
| **Outcome Title**      | Short, active statement of the desired change (e.g., “Reduce onboarding time”)                                  |
| **Purpose / Why**      | The strategic, user-centered, or operational reason this outcome matters                                        |
| **Signals of Success** | Metrics or qualitative observations that indicate progress (can include both quantitative and qualitative data) |
| **Constraints**        | Fixed boundaries such as legal, ethical, or technical limits                                                    |
| **Quality Attributes** | Specific attributes that define what “good” looks like (e.g., usability, reliability)                           |
| **Team Owner(s)**      | Who is responsible for exploring and delivering this outcome                                                    |
| **Current Confidence** | A self-assessed level (e.g., High, Medium, Low) based on available evidence and clarity                         |
| **Discovery Notes**    | Recent learning, key hypotheses, or active experiments related to this outcome                                  |
| **Target Timeframe**   | A flexible time horizon for when meaningful signals should emerge                                               |

#### Example Outcome Card

```
**Title:** Reduce Onboarding Time for New Customers

- **Purpose:** Improve time-to-value and reduce churn risk
- **Signals:** Avg onboarding duration (quant), support calls (qual),
  feedback sentiment
- **Constraints:** Legal (KYC compliance), platform limitations
- **Quality Attributes:** Simplicity, reliability
- **Owner:** Pod Delta
- **Current Confidence:** Medium – discovery ongoing
- **Discovery Notes:** Testing new flow, user interviews show friction
  in identity check
- **Target Timeframe:** 60 days
```

#### Using Outcome Cards in Practice

* Teams should create and regularly update cards as part of planning,
  discovery, and delivery.
* Leadership can review Outcome Cards in place of traditional project
  plans to track learning and outcome progress.
* A shared **Outcome Portfolio** can be created by organizing multiple
  cards across strategic themes or domains, enabling cross-team
  alignment at scale.

### Outcome vs. Output

A frequent challenge in applying outcome-based thinking is the
**blurring between outcomes and outputs**, especially at the team
level. Outputs are **things we deliver**—features, services,
improvements. Outcomes are the **observable changes** those outputs
create in the real world.

> An output is **delivered**.  
> An outcome is **validated** by real-world evidence.

Teams often mistake improved functionality (e.g., "faster page load")
for outcomes, when the true goal is behavioral or value-based (e.g.,
"more users complete the checkout process").

#### Outcome vs. Output Examples

| **Outcome** (Change in behavior or value) | **Signals** (Evidence of change)                 | **Outputs** (Delivered items)                    |
| ----------------------------------------- | ------------------------------------------------ | ------------------------------------------------ |
| Users complete tasks faster               | Task completion rate, drop-off reduction         | Page speed optimization, simplified UI steps     |
| Increase self-service account setup       | % of accounts created without human intervention | Redesigned onboarding flow, automated support    |
| Fewer password-related support tickets    | Support volume, user feedback                    | “Forgot Password” UX update, validation system   |
| Greater trust in billing                  | Reduced billing complaints, improved NPS         | Transparent invoice design, usage tooltips       |
| Increased usage of scheduling feature     | Feature adoption rate, repeat usage              | Feature release, onboarding prompt, help content |

> **Example:**
> “Improve page load time” is an **output** (a quality attribute).
> “Increase checkout completion rate” is the **outcome**. The former
> contributes to the latter, but **is not the outcome itself**.

#### Tip for Teams

When defining an outcome, ask:

* What will people **do differently** if this works?
* How will we **observe or measure** that change?
* Could the outcome be achieved **in multiple ways**?

Outcomes describe **why something matters** and how success is
observed—not what to build.

### Cross-Team Coordination and Outcome Ownership

Outcomes should not be split across teams with different
priorities. To ensure coherence and ownership:

* Each outcome should belong to one clearly defined team or pod.
* If multiple teams contribute, they must act as a single
  outcome-focused unit.
* Organizational structures should evolve to reflect outcome
  boundaries (Conway’s Law in reverse).
* Interfaces and dependencies should be framed as contracts, not
  coordination burdens.

Teams are encouraged to reorganize when outcome ownership becomes
diluted or coordination overhead increases.

### Leadership and Oversight

Leadership in OBAF means enabling, not directing. Oversight should be
nearly invisible:

* **Optimal oversight is automated**: KPIs, usage data, or real-world
  impact signals.
* Leaders guide through vision and constraints—not feature lists or
  status reports.
* Interventions should only occur in cases of systemic failure,
  ethical risk, or learning breakdowns.
* Sponsors and managers should practice servant leadership —
  understood here as a facilitative approach — by funding experiments,
  supporting outcome framing, and modeling evidence-based
  decision-making.

#### From Status-Reporting to Evidence-Framing

For leaders and sponsors, shifting from traditional output oversight
to outcome enablement means more than changing what gets tracked—it
requires changing how conversations happen.

In status-reporting cultures, reviews often focus on surface-level
indicators: percent complete, story points burned, or tasks
delivered. These measures are easy to collect but rarely illuminate
whether meaningful progress is happening.

Evidence-framing transforms these conversations. Instead of asking,
*“Are we on track?”* leaders ask, *“What have we learned?”*, *“What
signals are emerging?”*, and *“What’s the current level of confidence
in the outcome?”*

This shift happens progressively, often in small steps:

* A delivery update evolves into a learning review, where the team
  shares new insights about user behavior or system feedback—not just
  what was built.
* Instead of tracking feature completion, leaders start watching for
  real-world signals that value is emerging (e.g., user adoption,
  friction reduction, behavior change).
* Weekly reports stop listing tasks and start summarizing discoveries,
  test results, and adjustments based on evidence.
* "Red-yellow-green" status summaries give way to qualitative
  confidence levels—rooted in both data and team insight.

As leaders adopt this posture, they stop asking for certainty and
start investing in clarity. They stop steering by roadmap and start
enabling exploration, grounded in trust and observable impact.

The goal is not to eliminate accountability—but to make it
meaningful. When teams are asked to show **evidence of learning**, not
just activity, accountability becomes a tool for alignment, not
control.

### Recommended Team Flow: Exploratory Kanban

To support daily exploratory work in Outcome-Based Agile, teams are
encouraged to adopt a lightweight Kanban system that promotes focus,
flow, and learning across disciplines. This system is not limited to
software—it applies to any kind of outcome-focused work, including
research, operations, design, service development, or policy.

#### Suggested Columns

* **Ready for Development** – Items selected from the backlog, framed
  as hypotheses to explore
* **In Development** (or **Doing**, **In Process**, etc) – Items
  currently in active exploration, design, testing, or creation
* **Ready to Test** – Work paused for review or integration, awaiting
  evaluation
* **Test** – Evaluation of fitness, coherence, quality attributes, or
  integration with other efforts
* **Done** – Complete and ready for delivery, use, or deployment

A **Backlog** (or **To Do**) column contains the team's working
hypotheses. These are not verified answers but informed bets. When the
backlog becomes too shallow, it triggers a discovery or planning
session to replenish ideas grounded in the outcome’s intent and
constraints.

#### Pull System Across the Flow

The system operates as a pull-based workflow:

* **Work is never pushed forward.** Instead, each column **pulls**
  from the previous one when capacity allows and context is ready.
* When work in the **In Development** (or Doing) column is complete,
  it is moved to **Ready to Test**, not directly into **Test**. This
  ensures it is **explicitly pulled into validation** when the team
  has capacity and focus.
* Similarly, teams **pull work from the Backlog into Ready for
  Development**, ensuring prioritization is intentional and
  capacity-aware.

This pull mechanism prevents overload, respects WIP limits, and
encourages reflection at each transition. It reinforces that movement
between phases is a deliberate choice, not an automatic step.

#### WIP Limits

To support team autonomy, sustainable pace, and continuous learning,
the columns **Ready for Development**, **In Development**, and
**Test** should include **Work In Progress (WIP) limits**. These are
flexible boundaries set by the team to reveal bottlenecks, maintain
smooth flow, and foster intentional decision-making—rather than rigid
rules imposed externally.

#### Validation Happens Outside the Board

This board reflects internal work readiness and coordination—not
outcome success. Actual validation of a hypothesis happens through
**external metrics**, preferably **automated and continuous**, such as
behavior change, system performance, or user engagement. The **Done**
column indicates readiness for delivery, not proof of effectiveness.

#### Why This Flow Supports OBAF

This Exploratory Kanban system enables:

* **Visible exploration** of hypotheses
* **Team-managed flow** based on capacity, not forced schedules
* **Clear separation of working and validating**
* **Adaptive prioritization** grounded in outcomes
* **Continuous discovery** through parallel metrics and learning

It supports autonomy, encourages sustainable pace, and keeps delivery
aligned with real-world evidence—as Outcome-Based Agile demands.

### Validation Outside the Board

The Kanban board reflects the team’s internal readiness and
coordination—not the validation of actual outcomes.

* **Validation happens beyond the board**, through continuous,
  preferably automated, metrics (e.g., behavior change, system
  performance, user engagement).
* There are no predefined validation criteria before delivery, because
  outcomes cannot be fully anticipated.
* **Validation is emergent**, based on real-world feedback and
  production use—via A/B tests, before-after comparisons, or system
  telemetry.

Equally important, OBAF emphasizes qualitative validation. Insights
from user interviews, observational research, session recordings, and
open-ended feedback provide context and meaning that quantitative
signals alone cannot capture. These methods reveal why users behave
the way they do, not just what they do. Emerging technologies—such as
AI-powered sentiment analysis, natural language processing, and
real-time experience tracking—now make qualitative validation more
scalable and actionable than ever before.

The Outcome-Based Agile Framework treats both forms of evidence as
essential and complementary. Effective validation blends data and
dialogue, measuring behavior while understanding intent. This ensures
that outcomes are not only observable but meaningful.

### Common Anti-Patterns in Outcome Validation

To maintain the integrity of outcome-based work, teams must be mindful
of common traps that quietly erode learning and agility. These
anti-patterns often emerge when metrics are misused, constraints
become overly rigid, or activity is mistaken for value.

#### 1. Goodhart’s Law in Action

> When a measure becomes a target, it ceases to be a good measure.

When teams fixate on hitting specific numbers (e.g., reducing bounce
rate or increasing click-through), they may lose sight of the
underlying change that truly matters. Metrics should serve as signals
for exploration—not targets to hit at any cost.

#### 2. Vanity Metrics

Not all data is meaningful. Metrics like page views, impressions, or
internal velocity can create the illusion of progress while masking
stagnation. These vanity metrics look good on dashboards but rarely
reflect real user or business outcomes. Prioritize signals that tie
directly to meaningful behavior change or value realization.

#### 3. Constraint Creep

Constraints exist to define safe boundaries—not to prescribe
solutions. Over time, teams often inherit outdated specifications or
interpret vague standards as fixed requirements. This slow expansion
of what’s considered “non-negotiable” can quietly kill
innovation. Revisit constraints regularly to ensure they’re still
valid, contextual, and evidence-based.

**What to do instead:**

* Use metrics as **learning tools**, not success criteria. Let signals
  inform, not dictate.
* Question the purpose behind each measurement. Ask, *“What does this
  really tell us?”*
* Revalidate constraints. Treat them as hypotheses too—especially when
  they block experimentation.

These reminders help teams stay focused on learning and progress that
matters—so validation remains a discovery process, not a performance
ritual.

## Outcome-Based Agile Readiness Assessment

This *Readiness Assessment* focuses on organizational, team, and
leadership readiness across five key dimensions. Each area includes a
short description and 3 yes/no questions. Use it as a kickoff
diagnostic or self-check before adopting the framework.

### 1. Outcome Thinking

> Are we focused on value and behavior change, not just delivery?

- [ ] Do we define success in terms of user behavior, business value,
      or system performance—not just features delivered?
- [ ] Do we regularly ask *“Why does this matter?”* before *“What are
      we building?”*
- [ ] Do teams have a clear understanding of the desired outcome
      before starting work?

**If “yes” to 2 or more**: Outcome awareness is emerging.

### 2. Team Autonomy and Cross-Functionality

> Can teams own the problem, not just execute tasks?

- [ ] Do teams have the skills to explore and deliver without constant
      handoffs?
- [ ] Are teams trusted to make solution decisions within clear
      boundaries?
- [ ] Do teams work from outcomes or constraints, rather than
      prewritten tickets?

**If “yes” to 2 or more**: Autonomy foundations are in place.

### 3. Evidence-Driven Culture

> Are decisions grounded in learning from users and data?

- [ ] Do we treat plans as hypotheses that can change based on new
      learning?
- [ ] Are experiments, metrics, or real-world signals used to guide
      work?
- [ ] Do leaders welcome evidence that challenges assumptions?

**If “yes” to 2 or more**: Culture supports evidence-based work.

### 4. Psychological Safety and Learning

> Can people speak up, learn from failure, and improve continuously?

- [ ] Are retrospectives or reviews blameless, structured, and
      regularly held?
- [ ] Can team members safely admit mistakes or raise concerns?
- [ ] Are failures framed as learning opportunities, not performance
      gaps?

**If “yes” to 2 or more**: A learning environment exists.

### 5. Leadership as Enabler

> Do leaders guide through intent and remove blockers?

- [ ] Are leaders setting direction through desired outcomes—not
      features or task lists?
- [ ] Do managers act more as sponsors and coaches than controllers?
- [ ] Is governance light, adaptive, and supportive of learning and
      change?

**If “yes” to 2 or more**: Leadership is aligned with OBAF principles.

### Scoring Summary

* **4–5 areas with “2 or more YES answers”**: You're ready to pilot
  The Outcome-Based Agile Framework.
* **2–3 areas**: Start with a small team or experiment and invest in
  coaching.
* **0–1 areas**: Begin with mindset and cultural groundwork before
  rollout.

## Simple Process Overview

1. **Input:** Outcome + Constraints
2. **Process:** Discovery + Delivery (Iterative)
3. **Output:** Validated Experiments, Prototypes, and Releases
4. **Outcome:** Real-world change (measured)
5. **Feedback Loop:** After Action Reviews (AARs) inform learning and
   next steps

## Facilitation Guide

1. **Kickoff Workshop:** Define the outcomes, clarify the purpose
   behind each outcome, surface constraints, and establish team
   autonomy
2. **Cadence Design:** Establish short cycles of delivery and feedback
   (e.g., 1-2 weeks)
3. **Feedback Loops:** Embed continuous discovery methods (user
   interviews, A/B tests, analytics reviews)
4. **Review Rituals:** Use retrospectives in the form of AARs to focus
   on what was planned, what happened, why it happened, and what was
   learned (see Tenet 10 for full AAR structure)
5. **Leadership Coaching:** Train sponsors and managers to support
   outcome framing, not output control

## Key Reminders

* If you are writing a solution before exploring the problem, pause.
* If requirements are treated as fixed outputs, reframe them as
  boundaries.
* If retrospectives assign blame, reset the culture.
* If decisions ignore evidence, restart the discovery.

## Checklists

These ten checklists—each with five concise, actionable points—are
designed to support kickoff meetings, initial workshops, or the launch
of new outcomes within established teams. They help ensure clarity,
alignment, and momentum across a range of critical topics:

1. Clarity and Alignment on Outcomes
2. Lightweight Outcome Metrics
3. Discovery Embedded in the Work
4. Teams Own Outcomes, Not Tasks
5. Reframing Requirements as Constraints
6. Structuring After Action Reviews (AARs)
7. Governance That Enables, Not Controls
8. Organizing Around Shared Outcomes
9. Leadership as Outcome Enablers
10. Organizational Agility Support

### 1. Outcome Definition and Framing

- [ ] Is the outcome expressed as a change in behavior, business
      result, or system capability?
- [ ] Has the *purpose* of the outcome been made explicit (e.g., why
      it matters to users, teams, or strategy)?
- [ ] Have the relevant quality attributes been identified (e.g.,
      usability, security)?
- [ ] Is the outcome flexible in *how* it’s achieved, but firm in
      *why* it matters?
- [ ] Have success signals (qualitative or quantitative) been defined?

### 2. Measurement and Evidence**

- [ ] Is there a measurable signal for outcome progress (e.g., a
      proxy, heuristic, or direct metric)?
- [ ] Can the signal be tracked continuously or in short cycles?
- [ ] Are stakeholders aligned on what “evidence” looks like?
- [ ] Are teams using the data to adjust their approach, not just
      report status?
- [ ] Are metrics viewed as signals to explore, not KPIs to hit
      blindly?

### 3. Continuous Discovery Integration

- [ ] Are discovery activities happening in parallel with delivery?
- [ ] Is learning from experiments (e.g., A/B tests, prototypes)
      captured?
- [ ] Do discovery insights influence next steps or pivot decisions?
- [ ] Are assumptions treated as hypotheses, not facts?
- [ ] Are users and stakeholders engaged early and often?

### 4. Team Autonomy and Problem Ownership

- [ ] Do teams start with a problem, not a backlog of predefined
      tasks?
- [ ] Can teams adjust scope and solutions to achieve outcomes?
- [ ] Are cross-functional skills available within the team?
- [ ] Is problem framing part of team planning?
- [ ] Are teams trusted to challenge the framing of outcomes if
      needed?

### 5. Constraints and Boundaries

- [ ] Have non-negotiable constraints (technical, regulatory, ethical)
      been documented?
- [ ] Are constraints used to shape exploration, not dictate outputs?
- [ ] Is there a shared understanding of what can and cannot change?
- [ ] Are teams enabled to discover the best solution within those
      constraints?
- [ ] Do constraints evolve with discovery if context changes?

### 6. Blameless Retrospectives and Learning Culture

- [ ] Are retrospectives structured around *What was planned? What
      happened? Why? What did we learn?*
- [ ] Is participation inclusive and non-hierarchical?
- [ ] Are learnings documented without names or blame?
- [ ] Are multiple perspectives explored without forcing consensus?
- [ ] Do learnings influence future behavior or decisions?

### 7. Governance and Simplicity

- [ ] Are governance practices light and fit-for-purpose?
- [ ] Are teams free to make technical and design decisions within
      boundaries?
- [ ] Is governance focused on learning, not compliance?
- [ ] Are outcome reviews prioritized over milestone checklists?
- [ ] Are governance structures adaptive as context changes?

### 8. Coordination Across Teams

- [ ] Are outcomes divided in a way that minimizes cross-team
      dependencies?
- [ ] Do teams exposing interfaces (e.g., APIs, services) treat them
      as contracts?
- [ ] Is communication structured to match system architecture
      (Conway’s Law)?
- [ ] Are shared discovery efforts conducted for cross-cutting
      concerns?
- [ ] Are teams encouraged to re-org when friction emerges?

### 9. Leadership and Sponsorship

- [ ] Are leaders setting vision and outcomes, not dictating features?
- [ ] Do they reward learning, even when it disproves assumptions?
- [ ] Are they present in retrospectives or discovery reviews?
- [ ] Are managers being coached on adaptive leadership practices?
- [ ] Do they protect space for teams to explore and learn?

### 10. Cultural Readiness and Adaptation

- [ ] Is psychological safety actively cultivated?
- [ ] Are success and failure both treated as learning opportunities?
- [ ] Are language and metaphors aligned to outcome thinking (e.g.,
      “What problem are we solving?”)?
- [ ] Is process adaptation encouraged over adherence to rituals?
- [ ] Is evidence given more weight than authority or tradition?

## Applying OBAF in Regulated Environments

> This guidance exists to help reformers apply outcome-based thinking
> in **non-ideal contexts**, not to justify legacy approaches.

This complementary checklist is intended for teams operating in highly
regulated environments that require adherence to contracting
constraints, phase-gate processes, or sequential development cycles
such as the V-model. Although these conditions are not ideal for agile
practices, the checklist helps incorporate outcome-based agile
thinking within these limitations. It also assists in identifying
where existing constraints support or conflict with OBAF principles,
encouraging a gradual shift toward more outcome-focused contracting
philosophies and practices.

### 1. Frame Regulatory Needs as Immutable Constraints

- [ ] Have all mandatory regulations, standards, or audit requirements
      been clearly identified and documented as *non-negotiable
      constraints*?
- [ ] Are these constraints understood by the team as guardrails, not
      deliverables?

### 2. Treat Compliance Evidence as Outcomes

- [ ] Is the evidence of compliance (e.g., traceability, testing
      coverage, documentation) framed as part of the outcome
      definition?
- [ ] Are compliance deliverables produced iteratively and integrated
      into the workflow, not left to the end?

### 3. Enable Discovery Within the Boundaries

- [ ] Have areas where flexibility is *still possible* (e.g.,
      usability, automation, user workflow) been clearly defined to
      allow innovation?
- [ ] Are hypotheses and solution exploration encouraged within the
      compliance envelope?

### 4. Automate Traceability and Documentation

- [ ] Are tools or lightweight methods in place to automatically
      capture decision logs, test coverage, change history, or
      requirement links?
- [ ] Are compliance artifacts continuously validated instead of
      batch-generated?

### 5. Engage Risk and Compliance as Learning Partners

- [ ] Are compliance/risk officers included early in framing outcomes
      and reviewing experiments?
- [ ] Is the compliance team encouraged to collaborate in defining
      what *evidence* of safety or control looks like—rather than
      dictating process?

## Conclusion

Outcome-Based Agile is not a methodology; it is a recommitment to what
agility was always meant to be: adaptive, user-centered, and
value-focused. It is a call to drop the illusion of control in favor
of the pursuit of clarity, progress, and real-world impact.

**Outcomes, not outputs. Always.**

## Signatories

* *Michel Blomgren <sa6mwa@gmail.com> (2025-04-26)*
